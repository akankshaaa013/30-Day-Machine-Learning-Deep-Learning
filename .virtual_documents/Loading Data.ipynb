


!pip install scikit-learn
!pip install pandas
!pip install matplotlib
!pip install pandavro
!pip install sqlalchemy


from sklearn import datasets
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
import pandavro as pdx
import requests
from sqlalchemy import create_engine





digits = datasets.load_digits()
digits.data


features = digits.data
target = digits.target


features.shape





# Contains150 observations on the measurement of Iris flowers.
data = datasets.load_iris()
print(data.DESCR)


# Contains 569 observations of the breast cancer wisconsin dataset
data = datasets.load_breast_cancer()
print(data.DESCR)





from sklearn.datasets import make_regression
# Generate feature matrix, target vector, and true coefficients.
features, target, coeff = make_regression(
    n_samples = 100,
    n_features = 3,
    n_informative = 3,
    n_targets = 1,
    noise = 0,
    coef = True,
    random_state = 1
)

print("Feature Matrix\n", features[:10])
print("\nTarget Vector\n", target[:10])
print("\nCoefficients\n",coeff)


# Datasets designed to work well with the Classification Techniques.

from sklearn.datasets import make_classification
# Generate feature matrix, target vector, and true coefficients.
features, target = make_classification(
    n_samples = 100,
    n_features = 5,
    n_informative = 3,
    n_redundant = 1,
    n_classes = 3,
    weights = [.25,.45,.30],
    random_state = 1
)
print("Feature Matrix\n", features[:10])
print("\nTarget Vector\n", target[:10])


# Datasets designed to work well with the Clustering Techniques.

from sklearn.datasets import make_blobs
# Generate feature matrix, target vector, and true coefficients.
features, target = make_blobs(
    n_samples = 200,
    n_features = 2,
    centers = 5,
    cluster_std = 0.5,
    shuffle = True,
    random_state = 1
)
print("Feature Matrix\n", features[:10])
print("\nTarget Vector\n", target[:10])


# Using matplotlib, we can visualize the clusters generated by make_blobs.
plt.scatter(features[:,0], features[:,1], c=target)
plt.show()
plt.tight_layout()





url = 'https://raw.githubusercontent.com/chrisalbon/sim_data/master/data.csv'
df = pd.read_csv(url)
df.head()





url = 'https://raw.githubusercontent.com/chrisalbon/sim_data/master/data.xlsx'
df = pd.read_excel(url, sheet_name=0, header=0)
df.head()





url = 'https://raw.githubusercontent.com/chrisalbon/sim_data/master/data.json'
df = pd.read_json(url, orient="columns")
df.head()





url = 'https://machine-learning-python-cookbook.s3.amazonaws.com/data.parquet'
df = pd.read_parquet(url)
df.head()





url = 'https://machine-learning-python-cookbook.s3.amazonaws.com/data.avro'

response = requests.get(url)
if response.status_code == 200:
    with open("data.avro","wb") as f:
        f.write(response.content)
else:
    print(f"Failed to download the file. HTTP Status Code: {response.status_code}")

df = pdx.read_avro("data.avro")
df.head()





database_connection = create_engine("sqlite:///sample.db")
data = pd.read_sql_query("SELECT  * FROM data", database_connection)





url = "https://docs.google.com/spreadsheets/d/1ehC9otcAuitqnmWksqt1mOrTRCL38dv0K9UjhwzTOA/export?format=csv"
data = pd.read_csv(url)





url = "https://machine-learning-python-cookbook.s3.amazonaws.com/text.txt"
response = requests.get(url)
if response.status_code == 200:
    with open("text.txt","wb") as f:
        f.write(response.content)

with open("text.txt", "r") as f:
    text = f.read()

print(text)






