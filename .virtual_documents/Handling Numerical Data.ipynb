


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification, make_blobs
from sklearn.covariance import EllipticEnvelope
from sklearn.cluster import KMeans
from sklearn.impute import KNNImputer

import warnings
warnings.filterwarnings("ignore")


feature = np.array([
    [-500.5],
    [-24.9],
    [0],
    [1500],
    [860],
    [100.9],
    [-900]
])
minmax = preprocessing.MinMaxScaler(feature_range = (0, 1))
scaled_feature = minmax.fit_transform(feature)
scaled_feature


using_formula = (feature - feature.min())/(feature.max() - feature.min())
using_formula





standard_scaler = preprocessing.StandardScaler()
standardized = standard_scaler.fit_transform(feature)
standardized


# Using formula
(feature - feature.mean())/feature.std()


print("Mean of Scaled Feature", standardized.mean().round(0))
print("Standard Deviation of Scaled Feature", standardized.std().round(0))





robust_scaler = preprocessing.RobustScaler()
robust_scaler.fit_transform(feature)





X, y = make_classification(
    n_samples = 1000,
    n_features = 2,
    n_redundant = 0,
    n_clusters_per_class = 1,
    weights = [0.35, 0.65],
    random_state = 42
)
X[:, 0] = (X[:, 0] * 10)
X[:, 1] = (X[:, 1] * 72232)

X_train, X_test, Y_train, Y_test = train_test_split(X, y, random_state = 42, test_size = 0.3)

def apply_scaling_and_evaluate(scaler):
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    model = LogisticRegression(random_state = 42)
    model.fit(X_train_scaled, Y_train)
    prediction = model.predict(X_test_scaled)
    accuracy = accuracy_score(Y_test, prediction)

    plt.scatter(X_train_scaled[:,0], X_train_scaled[:, 1], c = Y_train, cmap = 'viridis', alpha = 0.5)
    plt.title(f"{scaler.__class__.__name__} = Accuracy: {accuracy:.2f}")
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.show()

plt.scatter(X_train[:,0], X_train[:, 1], c = Y_train, cmap = 'viridis', alpha = 0.5)
plt.title(f"Original Data")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()

scalers = [preprocessing.MinMaxScaler(), preprocessing.StandardScaler(), preprocessing.RobustScaler(), preprocessing.MaxAbsScaler()]

for scaler in scalers:
    apply_scaling_and_evaluate(scaler)





features = np.array([
    [0.5, 0.54],
    [1.34, 0.99],
    [8, 4],
    [12, 6.666],
    [1.4, 67.8]
])
normalizer = preprocessing.Normalizer(norm = "l1")
normalizer.fit_transform(features)





features = np.array([
    [2, 3, 4],
    [5, 6, 7],
    [3, 4, 5]
])

polynomial_interaction = preprocessing.PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False)
polynomial_interaction.fit_transform(features)
# Restrict the features created only to interaction features.





def add_ten(x : int) -> int:
    return x + 10

ten_transformer = preprocessing.FunctionTransformer(add_ten)
ten_transformer.fit_transform(features)


# We can do the same transformation in pandas using apply() method.
df = pd.DataFrame(features, columns = ["feature 1", "feature 2", "feature 3"])
df.apply(lambda x : x + 10)





features, _ = make_blobs(
    n_samples = 10,
    n_features = 2,
    centers = 1,
    random_state = 1
)
# Introducing the Outliers
features[0,0] = 826338
features[0,1] = 242338
features[4,0] = 110638
features[4,1] = 291138

features


outlier_detect = EllipticEnvelope(contamination = .2)
outlier_detect.fit(features)
outlier_detect.predict(features)





def indices_of_outliers(x : int) -> np.array(int):
    q1, q3 = np.percentile(features, [25, 75])
    iqr = q3-q1
    lower_bound = q1 - (iqr * 1.5)
    upper_bound = q3 + (iqr * 1.5)
    return np.where((x > upper_bound) | (x < lower_bound))

indices_of_outliers(features)





houses = pd.DataFrame()
houses["Price"] = [12345, 56343, 99127, 1278232, 32764, 13112]
houses["Rooms"] = [3, 4, 2, 100, 5, 1]
houses["Square_ft"] = [900, 1500, 1800, 18000, 1200, 1000]


houses[houses["Rooms"] < 20]


houses["Outliers"] = np.where(houses["Rooms"] < 20, 0, 1)
houses


# Transform the feature to dampen the effect of outlier
houses["Log_of_sqft"] = [np.log(x) for x in houses["Square_ft"]]
houses








# Binarize the feature according to some threshold
x = np.array([[60], [12], [20], [6], [50], [5], [45]])
binarizer = preprocessing.Binarizer(threshold = 18)
binarizer.fit_transform(x)


# Break up the features according to the multiple thresholds
np.digitize(x, bins = [20, 30, 55, 64])





features, _ = make_blobs(n_samples = 50,
                     n_features = 2,
                     centers = 3,
                     random_state = 1)
df = pd.DataFrame(features, columns = ["feature 1", "feature 2"])
clusterer = KMeans(3, random_state = 0)
clusterer.fit(features)


df["group"] = clusterer.predict(features)
df.head()





features = np.array([[1.1, 11.1],
                    [2.2, 22.2],
                    [3.4, np.nan],
                    [98, 32],
                    [np.nan, 21],
                    [np.nan, np.nan]])


features[~np.isnan(features).any(axis=1)]


df = pd.DataFrame(features, columns = ["feature1", "feature2"])

df.dropna()





features, _ = make_blobs(n_samples = 10000,
                        n_features = 2, random_state = 1)
scaler = preprocessing.StandardScaler()
standardized = scaler.fit_transform(features)
true_value = standardized[0,0]
standardized[0,0] = np.nan


knn_imputer = KNNImputer(n_neighbors = 5)
features_knn_imputed = knn_imputer.fit_transform(standardized)
print("True Value: ", true_value)
print("Imputed Value: ", features_knn_imputed[0,0])






