{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:58:14.372491Z","iopub.execute_input":"2025-01-01T10:58:14.372877Z","iopub.status.idle":"2025-01-01T10:58:14.378018Z","shell.execute_reply.started":"2025-01-01T10:58:14.372843Z","shell.execute_reply":"2025-01-01T10:58:14.376738Z"}},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"<h2 style=\"font-size:28px;font-family:Calibri\">\n    Fitting a Line\n</h2>\nYou want to train a model that represents a linear relationship between the\n feature and target vector.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"features, target = make_regression(n_samples = 100,\n                                  n_features = 3,\n                                  n_informative = 2,\n                                  n_targets = 1,\n                                  noise = 0.2,\n                                  coef = False)\n\nregr = LinearRegression()\nmodel = regr.fit(features, target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:23:40.077999Z","iopub.execute_input":"2025-01-01T10:23:40.078480Z","iopub.status.idle":"2025-01-01T10:23:40.104844Z","shell.execute_reply.started":"2025-01-01T10:23:40.078450Z","shell.execute_reply":"2025-01-01T10:23:40.104003Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"model.intercept_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:24:48.784288Z","iopub.execute_input":"2025-01-01T10:24:48.784639Z","iopub.status.idle":"2025-01-01T10:24:48.791333Z","shell.execute_reply.started":"2025-01-01T10:24:48.784610Z","shell.execute_reply":"2025-01-01T10:24:48.790281Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"0.009458002437218838"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"model.coef_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:24:56.865507Z","iopub.execute_input":"2025-01-01T10:24:56.865885Z","iopub.status.idle":"2025-01-01T10:24:56.872143Z","shell.execute_reply.started":"2025-01-01T10:24:56.865844Z","shell.execute_reply":"2025-01-01T10:24:56.870915Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"array([-1.34923155e-02,  1.13547012e+01,  9.21506920e+01])"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"print(f\"Actual Value: {target[0]}\")\nprint(f\"Predicted Value: {model.predict(features)[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:26:09.089704Z","iopub.execute_input":"2025-01-01T10:26:09.090097Z","iopub.status.idle":"2025-01-01T10:26:09.095962Z","shell.execute_reply.started":"2025-01-01T10:26:09.090066Z","shell.execute_reply":"2025-01-01T10:26:09.094928Z"}},"outputs":[{"name":"stdout","text":"Actual Value: 6.085854985752962\nPredicted Value: 6.203298265256171\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"print(model.score(features, target))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:27:05.132196Z","iopub.execute_input":"2025-01-01T10:27:05.132525Z","iopub.status.idle":"2025-01-01T10:27:05.138947Z","shell.execute_reply.started":"2025-01-01T10:27:05.132498Z","shell.execute_reply":"2025-01-01T10:27:05.137686Z"}},"outputs":[{"name":"stdout","text":"0.9999968656861672\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"<h2 style=\"font-size:28px;font-family:Calibri\">\n    Handling Interactive Effects\n</h2>\n Sometimes a feature’s effect on our target variable is at least partially\n dependent on another feature. For example, imagine a simple coffee-based\n example where we have two binary features— the presence of sugar\n (sugar) and whether or not we have stirred (stirred)—and we want to\n predict if the coffee tastes sweet. Just putting sugar in the coffee\n (sugar=1, stirred=0) won’t make the coffee taste sweet (all the\n sugar is at the bottom!) and just stirring the coffee without adding sugar\n (sugar=0, stirred=1) won’t make it sweet either. Instead it is the\n interaction of putting sugar in the coffee and stirring the coffee\n (sugar=1, stirred=1) that will make a coffee taste sweet. The effects\n of sugar and stirred on sweetness are dependent on each other. In this\n case we say there is an interaction effect between the features sugar and\n stirred.","metadata":{}},{"cell_type":"code","source":"features, target = make_regression(n_samples = 100,\n                                  n_features = 2,\n                                  n_informative = 2,\n                                  n_targets = 1,\n                                  noise = 0.2,\n                                  coef = False)\n\ninteraction = PolynomialFeatures(degree = 3, include_bias = False, interaction_only = True)\nfeatures_interaction = interaction.fit_transform(features)\nregr = LinearRegression()\nmodel = regr.fit(features_interaction, target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:31:55.818762Z","iopub.execute_input":"2025-01-01T10:31:55.819160Z","iopub.status.idle":"2025-01-01T10:31:55.827656Z","shell.execute_reply.started":"2025-01-01T10:31:55.819131Z","shell.execute_reply":"2025-01-01T10:31:55.826591Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"features[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:35:28.975395Z","iopub.execute_input":"2025-01-01T10:35:28.975756Z","iopub.status.idle":"2025-01-01T10:35:28.982948Z","shell.execute_reply.started":"2025-01-01T10:35:28.975725Z","shell.execute_reply":"2025-01-01T10:35:28.981796Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"array([-1.39117718, -2.1925686 ])"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"print(np.multiply(features[:, 0], features[:, 1])[0])\nprint(features_interaction[0, 2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:38:39.981020Z","iopub.execute_input":"2025-01-01T10:38:39.981377Z","iopub.status.idle":"2025-01-01T10:38:39.987484Z","shell.execute_reply.started":"2025-01-01T10:38:39.981346Z","shell.execute_reply":"2025-01-01T10:38:39.986279Z"}},"outputs":[{"name":"stdout","text":"3.0502514052056817\n3.0502514052056817\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"features_interaction[0, 2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:38:03.735127Z","iopub.execute_input":"2025-01-01T10:38:03.735507Z","iopub.status.idle":"2025-01-01T10:38:03.742980Z","shell.execute_reply.started":"2025-01-01T10:38:03.735471Z","shell.execute_reply":"2025-01-01T10:38:03.741698Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"3.0502514052056817"},"metadata":{}}],"execution_count":28},{"cell_type":"markdown","source":"<h2 style=\"font-size:28px;font-family:Calibri\">\n    Fitting a Non-Linear Relationship\n</h2>\n Polynomial regression is an extension of linear regression to allow us to\n model nonlinear relationships.\n <br><br>\nHow are we able to use a linear\n regression for a nonlinear function? The answer is that we do not change\n how the linear regression fits the model, but rather only add polynomial\n features. That is, the linear regression does not “know” that the x2 is a\n quadratic transformation of x. It just considers it one more variable.","metadata":{"execution":{"iopub.status.busy":"2025-01-01T10:40:30.983389Z","iopub.execute_input":"2025-01-01T10:40:30.983749Z","iopub.status.idle":"2025-01-01T10:40:30.990065Z","shell.execute_reply.started":"2025-01-01T10:40:30.983719Z","shell.execute_reply":"2025-01-01T10:40:30.988736Z"}}},{"cell_type":"code","source":"features, target = make_regression(n_samples = 100,\n                                  n_features = 3,\n                                  n_informative = 2,\n                                  n_targets = 1,\n                                  noise = 0.2,\n                                  coef = False)\n\npoly = PolynomialFeatures(degree = 3, include_bias = False)\nfeatures_poly = poly.fit_transform(features)\nregr = LinearRegression()\nmodel = regr.fit(features_poly, target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:45:11.287144Z","iopub.execute_input":"2025-01-01T10:45:11.287481Z","iopub.status.idle":"2025-01-01T10:45:11.296476Z","shell.execute_reply.started":"2025-01-01T10:45:11.287455Z","shell.execute_reply":"2025-01-01T10:45:11.295526Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"print(features[0])\nprint(features[0] ** 2)\nprint(features[0] ** 3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:46:46.306111Z","iopub.execute_input":"2025-01-01T10:46:46.306459Z","iopub.status.idle":"2025-01-01T10:46:46.312506Z","shell.execute_reply.started":"2025-01-01T10:46:46.306430Z","shell.execute_reply":"2025-01-01T10:46:46.311489Z"}},"outputs":[{"name":"stdout","text":"[-0.67250152 -0.82540303  0.37223946]\n[0.45225829 0.68129017 0.13856222]\n[-0.30414439 -0.56233897  0.05157832]\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"features_poly[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:46:05.077977Z","iopub.execute_input":"2025-01-01T10:46:05.078526Z","iopub.status.idle":"2025-01-01T10:46:05.085935Z","shell.execute_reply.started":"2025-01-01T10:46:05.078478Z","shell.execute_reply":"2025-01-01T10:46:05.084593Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"array([-0.67250152, -0.82540303,  0.37223946,  0.45225829,  0.55508479,\n       -0.2503316 ,  0.68129017, -0.30724758,  0.13856222, -0.30414439,\n       -0.37329537,  0.16834838, -0.45816867,  0.20662446, -0.0931833 ,\n       -0.56233897,  0.25360308, -0.11436967,  0.05157832])"},"metadata":{}}],"execution_count":38},{"cell_type":"markdown","source":"<h2 style=\"font-size:28px;font-family:Calibri\">\n    Reducing the Variance with Regularization.\n</h2>","metadata":{}},{"cell_type":"code","source":" features, target = make_regression(n_samples = 100,\n                    n_features = 3,\n                    n_informative = 2,\n                    n_targets = 1,\n                    noise = 0.2,\n                    coef = False)\n\nscaler = StandardScaler()\nfeatures_standardized = scaler.fit_transform(features)\nregr = Ridge(alpha = 0.5)\nmodel = regr.fit(features_standardized, target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:51:38.778516Z","iopub.execute_input":"2025-01-01T10:51:38.778994Z","iopub.status.idle":"2025-01-01T10:51:38.793862Z","shell.execute_reply.started":"2025-01-01T10:51:38.778959Z","shell.execute_reply":"2025-01-01T10:51:38.792835Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"regr_cv = RidgeCV(alphas = [0.001, 0.01, 0.1, 1.0, 10.0])\nmodel_cv = regr_cv.fit(features_standardized, target)\nmodel_cv.coef_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:55:00.583481Z","iopub.execute_input":"2025-01-01T10:55:00.583847Z","iopub.status.idle":"2025-01-01T10:55:00.593213Z","shell.execute_reply.started":"2025-01-01T10:55:00.583789Z","shell.execute_reply":"2025-01-01T10:55:00.592131Z"}},"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"array([1.01317865e+02, 6.60634459e+01, 1.19885220e-03])"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"model_cv.alpha_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:55:08.527134Z","iopub.execute_input":"2025-01-01T10:55:08.527470Z","iopub.status.idle":"2025-01-01T10:55:08.533200Z","shell.execute_reply.started":"2025-01-01T10:55:08.527442Z","shell.execute_reply":"2025-01-01T10:55:08.532138Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"0.001"},"metadata":{}}],"execution_count":45},{"cell_type":"markdown","source":"<h2 style=\"font-size:28px;font-family:Calibri\">\n    Reducing Features with Lasso Regression\n</h2>","metadata":{"execution":{"iopub.status.busy":"2025-01-01T10:57:35.935853Z","iopub.execute_input":"2025-01-01T10:57:35.936257Z","iopub.status.idle":"2025-01-01T10:57:35.942766Z","shell.execute_reply.started":"2025-01-01T10:57:35.936228Z","shell.execute_reply":"2025-01-01T10:57:35.941323Z"}}},{"cell_type":"code","source":"scaler = StandardScaler()\nfeatures_standardized = scaler.fit_transform(features)\nregr = Lasso(alpha = 0.5)\nmodel = regr.fit(features_standardized, target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:59:30.546407Z","iopub.execute_input":"2025-01-01T10:59:30.546732Z","iopub.status.idle":"2025-01-01T10:59:30.556343Z","shell.execute_reply.started":"2025-01-01T10:59:30.546704Z","shell.execute_reply":"2025-01-01T10:59:30.555250Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"model.coef_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T10:59:48.003912Z","iopub.execute_input":"2025-01-01T10:59:48.004257Z","iopub.status.idle":"2025-01-01T10:59:48.010635Z","shell.execute_reply.started":"2025-01-01T10:59:48.004231Z","shell.execute_reply":"2025-01-01T10:59:48.009619Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"array([100.82717554,  65.57256353,  -0.        ])"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":" # Create lasso regression with a high alpha\n regression_a10 = Lasso(alpha=10)\n model_a10 = regression_a10.fit(features_standardized, target)\n model_a10.coef_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T11:00:00.588913Z","iopub.execute_input":"2025-01-01T11:00:00.589308Z","iopub.status.idle":"2025-01-01T11:00:00.598307Z","shell.execute_reply.started":"2025-01-01T11:00:00.589279Z","shell.execute_reply":"2025-01-01T11:00:00.596748Z"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"array([91.48909497, 56.23448297, -0.        ])"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}